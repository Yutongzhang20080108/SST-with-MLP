{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yutongzhang20080108/SST-with-MLP/blob/main/SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple try to accomplish the SST2 task.\n",
        "First we import the necessary APIs\n"
      ],
      "metadata": {
        "id": "dIvIShuxaBEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ObMk0t8XE6Bw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r3I3UhDFHUW",
        "outputId": "5f35e576-29c7-4744-8ded-274192966baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first dict of train_data is {'text': 'a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films', 'label': 1, 'label_text': 'positive'}\n",
            "The first dict of test_data is {'text': 'no movement , no yuks , not much of anything .', 'label': 0, 'label_text': 'negative'}\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_json(\"sample_data/train.jsonl\", lines=True).to_dict(\"records\")\n",
        "test_data = pd.read_json(\"sample_data/test.jsonl\", lines=True).to_dict(\"records\")\n",
        "print(f\"The first dict of train_data is {train_data[0]}\")\n",
        "print(f\"The first dict of test_data is {test_data[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create a function to split the original SST dataset into tokens by a pretrained BERT language model."
      ],
      "metadata": {
        "id": "cJvwQYx7aVqd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3vHAPU26JvFm"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"config\")\n",
        "max_length = 128\n",
        "def my_collate_fn(batch):\n",
        "  text = []\n",
        "  label = []\n",
        "  for sample in batch:\n",
        "    text.append(sample[\"text\"])\n",
        "    label.append(sample[\"label\"])\n",
        "\n",
        "  text_list = []\n",
        "  for texts in text:\n",
        "    encoded_text = tokenizer.encode(texts, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "    text_list.append(encoded_text)\n",
        "  tensor_text = []\n",
        "  for text in text_list:\n",
        "    text_tensor = torch.Tensor(text)\n",
        "    tensor_text.append(text_tensor)\n",
        "  tensor_text = pad_sequence(tensor_text, batch_first=True, padding_value=0)\n",
        "  label_tensor = torch.Tensor(label)\n",
        "  return {'text':tensor_text, \"label\":label_tensor}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3b-YWIg_Innr"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=my_collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, collate_fn=my_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IrjyyFuQo60",
        "outputId": "7710e52d-f972-4e16-e51b-30d68a3a5cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the text is torch.Size([8, 128])\n",
            "The size of the label is torch.Size([8])\n",
            "tensor([ 101., 1045., 2031., 2025., 2042., 2023., 9364., 2011., 1037., 3185.,\n",
            "        1999., 1037., 2146., 2051., 1012.,  102.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
            "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.])\n",
            "tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "for train in train_loader:\n",
        "  text, label = train[\"text\"], train[\"label\"]\n",
        "print(f\"The size of the text is {text.size()}\")\n",
        "print(f\"The size of the label is {label.size()}\")\n",
        "print(text[0,:])\n",
        "print(label[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have uploader all the necessay train and test data. And we are going to create a MLP as our main model"
      ],
      "metadata": {
        "id": "Ll-dpipjajR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JH3fEsiRCQe5"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class DL(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.stack = nn.Sequential(\n",
        "        nn.Linear(max_length, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 2)\n",
        "    )\n",
        "  def forward(self, input):\n",
        "    x = self.flatten(input)\n",
        "    logits = self.stack(x)\n",
        "    return logits\n",
        "model = DL()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the train and test loop"
      ],
      "metadata": {
        "id": "K2HCgTnUaxbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "prbdU7-AE_De"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-2\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "\n",
        "def trainloop(dataloader, optim, loss_fn, model):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, train in enumerate(dataloader):\n",
        "    pred = model(train['text'])\n",
        "    loss = loss_fn(pred, train[\"label\"].to(torch.int64))\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "  if batch % 100 == 0:\n",
        "    loss, current = loss.item(), (batch + 1)*(len(train[\"label\"]))\n",
        "    print(f\"loss:{loss}, current:[{current}/{size}]\")\n",
        "def testloop(dataloader, model, loss_fn):\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs in dataloader:\n",
        "          pred = model(inputs[\"text\"])\n",
        "          test_loss += loss_fn(pred, inputs[\"label\"].to(torch.int64)).item()\n",
        "          correct += (pred.argmax(1) == inputs[\"label\"]).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to start training now!!!"
      ],
      "metadata": {
        "id": "f-Jyv0msa1be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RqXN1WFVMt",
        "outputId": "bec8d114-95bc-4246-bf8e-74b89909a5de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 0.693658 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.696987 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.699184 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.694391 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 49.9%, Avg loss: 0.694380 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    trainloop(train_loader, optimizer, loss_fn, model)\n",
        "    testloop(test_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8cDR5wHpWVen/KLywMsPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}